---
title: "Exporta vs No exporta con K-NN"
output: html_notebook
date: 2020-09-15
author: "Nicolás Stumberger"
---

Con el objetivo de aplicar el algoritmo de clasificiación denominado k-NN sobre datos reales, llevé adelante este análisis que tiene la intención de predecir si una empresa va a realizar una exportación (o no) durante el próximo año en base a variables que, puede suponerse, tienen cierta incidencia en la performance de las exportaciones de una empresa.

# Indice

1.  Sobre k-NN
2.  Sobre los datos
3.  Pasos

3.1. Obtención de datos\
3.2. Exploración y preparación de datos\
3.3. Entrenando el modelo\
3.4. Evaluando la performance del modelo\
3.5. Mejorando la performance del modelo

4.  Conclusiones y disparadores

# 1. Sobre el algoritmo k-NN

## "Dios los cría y ellos se juntan"

El algoritmo k-NN, por su siglas en inglés (k Nearest Neighbords: k Vecinos Cercanos) tiene por objetivo clasificar datos y colocarlos en la misma categoría de la de los "vecinos cercanos", por ser similares en otras caracterísitcas. Las cosas que son parecidas, comúnmente tienen propiedades parecidas. En este caso, la hipótesis sería que los exportadores parecidos tendrán un resultado parecido durante el próximo año. La "clasificación" es la etiqueta que queremos predecir, llamada "target feature".

Entonces, lo que hace este algoritmo es tomar observaciones sin clasificar (registros "nuevos") y les asigna la etiqueta de la clasificación de observaciones similares. Para encontrar esa similitud, se toma variables (features) disponibles.

Ahora bien, ¿cómo calcula el algoritmo la cercanía entre las observaciones (entre las empresas en este caso)? Sintéticamente, utiliza un cálculo para medir distancias entre las variables de cada observación, llamado "distancia euclidiana", el cual se deduce a partir del Teorema de Pitágoras.

No es foco de este análisis entrar en detalles sobre el algoritmo (ya que hay mucha información al respecto en la web), pero sí me interesa destacar un par de aspectos más. Primero, este algoritmo se lo describe como "lazy" (vago, perezoso), porque en realidad no produce un modelo (no se produce ninguna abstracción), sino que utiliza los mismos datos para realizar la clasificación y, por esto mismo, la posibilidad de entender cómo cada variable está relacionada con el resultado (clasificación) está bastante limitada. Por este motivo, este análisis no tendrá un resultado explicativo, sino que será únicamente predictivo.

Segundo, el algoritmo funciona únicamente con variables numéricas, por lo que cualquier variable categórica (si existiera, y en este caso existen) debe convertirse a una numérica creando variables "dummy" (proceso conocido como "dummy coding" o "one-hot encoding").

# 2. Sobre los datos

Los datos que se utilizaron para crear el análisis vienen una fuente privada que recopila datos de aduana. A los efectos de este análisis, las observaciones se agregaron a nivel anual por empresa. Estas últimas fueron anonimizadas. El rango de años tomados para el análisis es 2014-2019, utilizando el último año como target feature y no se lo utiliza para el algoritmo.

Existen límites respecto a la cantidad de variables que se pueden utilizar. La cantidad y qué variables cuenta la fuente primaria opera como uno de ellos. La otra restricción es más subjetiva y en este caso fue delimitada para que la etapa de preparación de datos, denominada "data wrangling" (o "data carpentry") no consuma demasiado tiempo, consiguiendo que este análisis nunca vea la luz.

Entre ambos límites, se encuentra este sweet spot (en mi opinión) de utilizar las variables mencionadas. Esto no quiere decir que la selección de atributos sea azaroza. Los mismos se sustentan en ciertas hipótesis sobre su relevancia en la performance exportadora de las empresas: diversificación de cartera de productos; diversificación de destinos; regiones más estables que otras como mercados de destino; industrias más estables que otras; volúmenes exportados.

Como resultado, las variables que se utilizaron para realizar este análisis son:

1.  Número (cantidad ) de posiciones arancelarias distintas a nivel NCM (por cada año analizado): 5 variables.
2.  Número (cantidad) de países distintos a los que se exportó (por cada año analizado): 5 variables.
3.  Número (cantidad) de países destino por región del mundo: Latinoamérica, Europa, Asia-Pacífico, Maghreb y Medio Oriente, Resto de África, Norteamérica, Oceanía, Comunidad de Estados Independientes y Otros: 8 variables.
4.  Gran rubro (variables dummy): las posiciones arancelarias exportadas a qué Gran Rubro (clasificación de INDEC) pertenece: Manufacturas de Origen Agrícola, Manufacturas de Origien Industrial, Productos Primarios, Combustible y Energía: 4 variables.
5.  Dólares FOB exportados por año: 5 variables

La descripción de este análisis no incluye todo el proceso de "data carpentry" realizado. Se dice que la etapa de preparación de datos ocupa más del 80% del proceso de análisis. Sin embargo, en este caso creo que fue cerca del 95%. Como el objetivo de este informe no es "data carpentry", consideré prescindible incluirlo y focalizarme en el tema del asunto.

# 3. Prediciendo si una empresa exportará (o no) el próximo año con k-NN

## 3.1. Carga de librerías

```{r}
library(tidyverse) # para manipular todos los datos
library(class) # paquete que contiene el algoritmo knn de clasificacion
library(gmodels) # para evaluar el modelo con crosstable
```

## 3.2. Obtención de datos

Tal como se mendiona arriba, los datos fueron procesados previamente. El resultado de ese data carpentry es el dataset exportadores\_prep.RDS.

```{r}
exportadores <- readRDS(file = "data/exportadores_prep.RDS")
```

El dataset cuenta con un poco más de 16.000 filas, representando exportadores, y 29 columnas. La primera es id del exportador, la segunda es el target feature (la variable que queremos predecir) que indica si la empresa exportó o no en el último año. El resto de las variables (27 ) son los atributos que utilizaremos para predecir.

```{r}
str(exportadores)
```

La tabla metadatos contiene la descripción de cada campo:

```{r}
metadatos <-
        xlsx::read.xlsx2(file = "data/metadatos.xlsx", 
                         sheetName = "metadatos")

metadatos
```

## 3.3. Preparación de datos

La primer variable es el id del exportador. Como es un identificador único por cada exportador, no provee ninguna información útil para el modelo, por lo que lo vamos a excluir.

```{r}
exportadores_1 <- exportadores %>% 
        select(-id)
```

La siguiente variable, target, es de particular importancia, ya que es el resultado que quiero predecir. Este campo indica si el exportador realizó o no una venta al exterior en el último año del periodo analizado, en este caso durante el año 2019.

```{r}
table(exportadores_1$target)
```

Se puede observar que de los 16 mil registros, aproximadamente la mitad realizó una exportación. EL dataset está balanceado.

```{r}
round(prop.table(table(exportadores_1$target)) * 100, digits = 1)
```

Las restantes 27 variables son numéricas, tal como el algoritmo requiere, y describen la cantidad de productos, cantidad de destinos, región del mundo a la que se exportó y montos despachados durante el rango de años estudiados.

A modo ilustrativo, se observa un resumen de 3 de estos campos: cantidad de productos distintos, cantidad de países distintos y valores exportados durante uno de los años.

```{r}
summary(exportadores_1[c("ncm_5", "dest_5", "fob_5")])
```

El algoritmo k-NN es altamente dependiente de la escala de las variables de input. Al analizar esta información en detenimiento , se puede ver que los rangos de valores de las variables ncm\_5 y dest\_5 son muy diferentes de la de fob\_5. Debido a que dest\_5 varía de 0 a 97 y fob\_5 varía de 0 a 2.904.000.000, el impacto de este último va a ser mayor en el cálculo de la distancia en el algoritmo, comparado con el primero. Por este motivo, el dataset debe ser normalizado.

### Normalización del dataset

Dispongo de dos alternativas para normalizar los datos:

-   Normalización con Min-Max

-   Normalización con Z-score (desviaciones estándar)

Ambos métodos son útiles para el agoritmo. La normalización min-max convierte todos los valores a una escala de 0 a 1, comprimiendo los extremos hacia el centro, acortando el rango total. La normalización con z-score, por su parte, no tiene límites mínimos ni máximos definidos, por lo que los valores extremos no son comprimidos hacia el centro si no que convierte la media del conjunto de valores en 0, y todos los valores se ubican por debajo o por encima del 0 en función a su distancia de la media, medida en desviaciones estándar.

La decisión para utilizar una u otra forma de normalizar los datos puede tomarse en base a los datos mismos: ¿interesa que pesen más los valores extremos o no?

Se comenzará utizando la normalización min-max. Para ello creo una función que luego me permita convertir todo el dataset.

```{r}
normalize <- function(x) {
        
        return((x - min(x, na.rm = TRUE)) / 
                       (max(x, na.rm = TRUE) - 
                                min(x, na.rm = TRUE)
                        )
               )
        
}
```

Testeo que la función esté funcionando bien.

```{r}
normalize(c(1,2,3,4, NA, 5))
normalize(c(10, 20, 30, 40, 50))
```

Ahora debo aplicar esta funcion a las 27 variables numericas del dataset. La función lapply me permite hacer eso. lapply toma una lista y aplica una función específica a cada elemento de la lista. Como una tabla es como una lista de vectores, podemos usar lapply. Por ultimo convertimos el resultado a un dataframe.

```{r}
# Excluyo la variable target para normalizar

exportadores_n <- lapply(exportadores_1[2:28], normalize) %>%
        as_tibble()


```

Para confirmar que se normalizaron bien, analizo el resumen estadístico de una variable.

```{r}
summary(exportadores_n$fob_5)
```

Perfecto, según lo esperado fob\_5 (exportaciones del último año de análisis sin contar el del target) tienen el rango 0-1.

### División del dataset en training y test

Tal como lo expresan las buenas prácticas del machine learning, vamos a separar el dataset en 2. Uno para entrenar el modelo y el otro para testearlo. La idea es que el modelo sea testeado sobre datos no vistos previamente por éste. En este caso, voy a utilizar el 80% de los registros para entrenar y el restante 20% para testear.

Para dividir el dataset, voy a crear una muestra aleatoria y replicable.

```{r}
# divido exportadores_n en expo_train y expo_test

# para poder replicar la mueastra aleatoria
set.seed(123)

# creo la muestra de un total de las filas del dataset, el 80%.
train_sample <- sample(nrow(exportadores_1), 
                       round(nrow(exportadores_1) * 0.8, 
                             digits = 0))


str(train_sample)

# creo el set de entrenamiento en base a exportadores_n, con las filas de la muestra y el set de test sin las filas de la muestra.
expo_train <- exportadores_n[train_sample,]
expo_test <- exportadores_n[-train_sample,]


```

Cuando se normalizó el dataset, se excluyó la variable target. Para entrenar el algoritmo, se necesita de esta variable, por lo que aquí la guardo en un vector para train y otro para test. Además chequeo que la proporción de "export" - "no\_export" de cada división sea similar al del dataset completo.

```{r}
expo_train_labels <- exportadores_1[train_sample, 1]
expo_test_labels <- exportadores_1[-train_sample, 1]

# Acá hequeo que la proporción de "export" o "no_export" sea similar a la del dataset completo (50%-50% aprox).

prop.table(table(expo_train_labels))

prop.table(table(expo_test_labels))

```

Bien, ahora está todo listo para entrenar el modelo.

## 3.4. Entrenando el modelo

Para un lazy learner como k-NN no es necesario ninguna contruccion de modelo. Solo hay que guardar la data de input en un formato estructurado, para usarlo con la nueva data.

La función knn() del paquete class identifica el k nearest neghbors, usando distancia Euclideana, en donde k es un numero especificado por el usuario y representa el número de vecinos a usar para decidir la clase de cada nuevo registro. El test clasifica mediante un voto entre los k nearest neighbors: se asigna la class de la mayoria de los vecinos y un empate se define aleatoreamente.

Algunos autores establecen que el número de k a utilizar debe ser igual a la raíz cuadrada del total de observaciones entrenadas.

```{r}

sqrt(nrow(expo_train)) %>% round(digits = 0)
```

En este caso, da 114. No obstante, este numero puede ajustarse en futuras iteraciones.

Ahora el algoritmo:

```{r}
# IMPORTANTE: cl DEBE ser vector, no table. Por lo tanto DEBE seleccionarse la columna!!!

expo_test_pred <-
        knn(
                train = expo_train,
                test = expo_test,
                cl = expo_train_labels$target,
                k = 114,
                prob = TRUE
        )

```

Esto devuelve un factor vector con los predicted labeles para el test

## 3.4. Evaluando la performance del modelo

Hacemos un cross table especificando prop.chisq = FALSE para quitar los valores chi-square del output.

```{r}
CrossTable(# nuevamente deben ser vectores
        x = expo_test_labels$target,
        y = expo_test_pred,
        prop.chisq = FALSE)

```

En esta tabla de arriba tenemos muchos valores. Las columnas representan las predicciones de clase y las filas las clases reales.

A continuación construyo una matriz de confusión un poco más casera para que me sirva para calcular el accuracy.

```{r}
# matriz de confusión casera para calcular el accuracy

confus_matrix <- table(expo_test_pred,expo_test_labels$target)

confus_matrix
```

Aquí las filas son las predicciones y las columnas representan la clasificación real.

Ahora creo una función para calcular el accuracy del modelo.

```{r}
# Esta función divide las predicciones correctas por el total de predicciones. Nos da el accuracy del modelo.

accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}

accuracy(confus_matrix)
```

76% de accuracy! Nada mal para ser el primer intento.

## 3.5. Mejorando la performance del modelo

# 4. Conclusiones y disparadores
