---
title: "Exportar o no exportar, ese es el dilema"
subtitle: "Prediciendo si una empresa exportará el próximo año con el algoritmo de clasificación k-NN"
output: 
        html_document:
        toc: TRUE
date: 2020-09-15
author: "Nicolás Stumberger"
---

Con el propósito de comenzar a incorporar técnicas de Machine Leanring al ámbito del comercio internacional, se desarrolló un modelo predictivo de clasificación para determinar si una empresa va a exportar o no el siguiente año.

El algoritmo utilizado es k-NN y las variables predictoras seleccionadas son: valores exportados, número y región de los destinos, número de productos diferentes y rubro al cual pertenece la empresa, tomando como horizonte temporal los últimos 5 años.

# 1. Sobre el algoritmo k-NN

## "Dios los cría y ellos se amontonan"

El algoritmo k-NN, por su siglas en inglés (k Nearest Neighbords: k Vecinos Cercanos) tiene por objetivo clasificar datos y colocarlos en la misma categoría de la de los "vecinos cercanos", por ser similares en otras caracterísitcas. Se basa en la hipótesis de que las cosas que son parecidas, comúnmente tienen propiedades similares. En este caso, la hipótesis bajada al caso práctico es que los exportadores parecidos (en su comportamiento exportador durante los últimos años) tendrán un resultado similar durante el el siguiente año. La "clasificación" es la etiqueta que se desea predecir, llamada "target feature", y en este análisis será binaria: exporta o no exporta durante el siguiente año.

En este sentido, lo que hace este algoritmo es tomar observaciones sin clasificar (registros "nuevos") y les asigna la etiqueta de la clasificación de observaciones similares, de las que sí se cuenta con su clasificación. Para encontrar esa similitud, se toma variables (features) disponibles.

Ahora bien, ¿cómo calcula el algoritmo la cercanía entre las observaciones (entre las empresas en este caso)? Sintéticamente, utiliza un cálculo para medir distancias entre las variables de cada observación, llamado "distancia euclidiana", el cual se deduce a partir del Teorema de Pitágoras.

No es foco de este análisis entrar en detalles sobre el algoritmo (ya que hay mucha información al respecto en la web), pero sí interesa destacar algunos aspectos que ayudarán a entender mejor el resultado. Primero, este algoritmo se lo describe como "lazy" (vago, perezoso), porque en realidad no produce un modelo (no se produce ninguna abstracción), sino que utiliza los mismos datos para realizar la clasificación y, por esto mismo, la posibilidad de entender cómo cada variable está relacionada con el resultado (clasificación) es limitada. Por este motivo, el análisis no tendrá un resultado explicativo, sino que será únicamente predictivo.

Segundo, el algoritmo funciona únicamente con variables numéricas, por lo que cualquier variable categórica (si existiera, y en este caso existen) debe convertirse a una numérica creando variables "dummy" (proceso conocido como "dummy coding" o "one-hot encoding").

Tercero, el algoritmo debe entrenarse y testearse sobre datos históricos para luego poder ser empleado para predecir años futuros. Se debe contar con el resultado real de la clasificación para poder determinar la exactitud del algoritmo. Por este motivo, se realiza un análisis retrospectivo, en este caso 2014-2019, utilizando el periodo 2014-2018 para las variables predictoras y el año 2019 como \*target feature\*. Pandemia a un lado, una vez entrenado y testeado el algoritmo sobre datos históricos, podría ser empleado para nuevos años (aún sin clasificar).

# 2. Sobre los datos

Los datos que se utilizaron para el estudio, se obtuvieron de una fuente privada que recopila datos de aduana. A los efectos del análisis, las observaciones se agregaron a nivel anual por empresa. Estas últimas fueron anonimizadas. El rango de años tomados para el análisis es 2014-2019, utilizando el último año como target feature y no se lo utilizó para el entrenamiento del algoritmo.

Existen límites respecto a la cantidad de variables que se pueden utilizar. Uno de ellos es la cantidad y qué variables cuenta la fuente primaria. La otra restricción es más subjetiva y en este caso fue delimitada para que la etapa de preparación de datos, denominada *data wrangling* (o *data carpentry*) y *feature engineering* no consuma demasiado tiempo.

Entre ambos límites, se encuentra el justo medio (licencia de autor) de utilizar las variables elegidas. Esto no quiere decir que la selección de atributos haya sido aleatoria, pero sí que se deja la puerta abierta a otras opiniones, modificaciones en los campos utilizados y propuestas de mejora relacionadas a la preparación de los predictores.

En defensa de las variables escogidas, éstas se sustentan en ciertas hipótesis sobre su relevancia en la performance exportadora de las empresas: diversificación de cartera de productos; diversificación de destinos; regiones más estables que otras como mercados de destino; industrias más estables que otras; volúmenes exportados; entre otras.

Como resultado de lo antedicho, las variables que se utilizaron para realizar este análisis son:

1.  Número (cantidad ) de posiciones arancelarias distintas a nivel NCM (por cada año analizado): 5 variables, una por año.
2.  Número (cantidad) de países distintos a los que se exportó (por cada año analizado): 5 variables, una por año.
3.  Número (cantidad) de países distintos por región del mundo a los que se exportó (duranto todo el periodo analizado de 5 años): Latinoamérica, Europa, Asia-Pacífico, Maghreb y Medio Oriente, Resto de África, Norteamérica, Oceanía, Comunidad de Estados Independientes y Otros: 8 variables, una por región.
4.  Gran rubro (variables dummy) al que pertenecen las posiciones arancelarias exportadas, según la clasificación de INDEC: Manufacturas de Origen Agrícola (MOA), Manufacturas de Origien Industrial (MOI), Productos Primarios (PP), Combustible y Energía (CyE): 4 variables, una por gran rubro.
5.  Dólares FOB exportados por cada año analizado: 5 variables, una por año.

La descripción de este estudio no incluye todo el proceso de *data carpentry* realizado. Se suele decir que la etapa de preparación de datos ocupa más del 80% del proceso de análisis. Sin embargo, en este caso, fue cerca del 95%. Como el objetivo del informe no es describir cómo se prepararon los datos, sino predecir exportaciones, se consideró prescindible incluirlo aquí y se priorizó el desarrollo del tema del asunto.

# 3. Prediciendo si una empresa va a exportar o no el siguiente año con k-NN

## 3.1. Carga de librerías

```{r librerias, message = FALSE, warning = FALSE}
library(tidyverse) # para manipular todos los datos, graficar, etc.
library(class) # paquete que contiene el algoritmo knn de clasificacion
library(gmodels) # para evaluar el modelo con crosstable
```

## 3.2. Obtención de datos

Tal como se mencionó arriba, los datos fueron procesados previamente. El resultado de ese *data carpentry* es el archivo exportadores\_prep.RDS.

```{r carga dataset}
exportadores <- readRDS(file = "data/exportadores_prep.RDS")
```

El dataset cuenta con poco más de 16.000 filas, representando exportadores, y 29 columnas. La primera es *id* del exportador, la segunda es el *target feature* (la variable que se desea predecir) que indica si la empresa exportó o no en el último año. El resto de las variables (27 ) son los atributos que se utilizarán para predecir.

```{r}
str(exportadores)
```

La tabla metadatos contiene la descripción de cada campo:

```{r}
metadatos <-
        xlsx::read.xlsx2(file = "data/metadatos.xlsx", 
                         sheetName = "metadatos")

knitr::kable(metadatos, caption = "Metadatos")
```

## 3.3. Preparación de datos

La primer variable es el id del exportador. Como es un identificador único por cada exportador, no provee ninguna información útil para el modelo, por lo que lo vamos a excluir.

```{r}
exportadores_1 <- exportadores %>% 
        select(-id)
```

La siguiente variable, target, es de particular importancia, ya que es el resultado que quiero predecir. Este campo indica si el exportador realizó o no una venta al exterior en el último año del periodo analizado, en este caso durante el año 2019.

```{r}
table(exportadores_1$target)
```

Se puede observar que de los 16 mil registros, aproximadamente la mitad realizó una exportación. El dataset está balanceado y es bueno para el tener un *baseline*: si usáramos solo el azar para predecir si una empresa exportará o no el próximo año, conociendo esta proporción, tenemos un 50% de probabilidades de acertarle al resultado.

```{r}
round(prop.table(table(exportadores_1$target)) * 100, digits = 1)
```

Las restantes 27 variables son numéricas, tal como el algoritmo requiere, y describen la cantidad de productos, cantidad de destinos, región del mundo a la que se exportó y montos despachados durante el rango de años estudiados.

A modo ilustrativo, se observa un resumen de 3 de estos campos: cantidad de productos distintos, cantidad de países distintos y valores exportados durante uno de los años.

```{r}
summary(exportadores_1[c("ncm_5", "dest_5", "fob_5")])
```

El algoritmo k-NN es altamente dependiente de la escala de las variables de input. Al analizar esta información en detenimiento , se puede ver que los rangos de valores de las variables ncm\_5 y dest\_5 son muy diferentes de la de fob\_5. Debido a que dest\_5 varía de 0 a 97 y fob\_5 varía de 0 a 2.904.000.000, el impacto de este último va a ser mayor en el cálculo de la distancia en el algoritmo, comparado con el primero. Por este motivo, el dataset debe ser normalizado.

### Normalización

Dispongo de dos alternativas para normalizar los datos:

-   Normalización con Min-Max

-   Normalización con Z-score (desviaciones estándar)

Ambos métodos son útiles para el agoritmo. La normalización min-max convierte todos los valores a una escala de 0 a 1, comprimiendo los extremos hacia el centro, acortando el rango total. La normalización con z-score, por su parte, no tiene límites mínimos ni máximos definidos, por lo que los valores extremos no son comprimidos hacia el centro si no que convierte la media del conjunto de valores en 0, y todos los valores se ubican por debajo o por encima del 0 en función a su distancia de la media, medida en desviaciones estándar.

La decisión para utilizar una u otra forma de normalizar los datos puede tomarse en base a los datos mismos: ¿interesa que pesen más los valores extremos o no?

Se comenzará utizando la normalización min-max. Para ello creo una función que luego me permita convertir todo el dataset.

```{r}
normalize <- function(x) {
        
        return((x - min(x, na.rm = TRUE)) / 
                       (max(x, na.rm = TRUE) - 
                                min(x, na.rm = TRUE)
                        )
               )
        
}
```

Testeo que la función esté funcionando bien.

```{r}
normalize(c(1,2,3,4, NA, 5))
normalize(c(10, 20, 30, 40, 50))
```

Ahora debo aplicar esta funcion a las 27 variables numericas del dataset. La función lapply me permite hacer eso. lapply toma una lista y aplica una función específica a cada elemento de la lista. Como una tabla es como una lista de vectores, podemos usar lapply. Por ultimo convertimos el resultado a un dataframe.

```{r}
# Excluyo la variable target para normalizar

exportadores_n <- lapply(exportadores_1[2:28], normalize) %>%
        as_tibble()


```

Para confirmar que se normalizaron bien, analizo el resumen estadístico de una variable.

```{r}
summary(exportadores_n$fob_5)
```

Perfecto, según lo esperado fob\_5 (exportaciones del último año de análisis sin contar el del target) tienen el rango 0-1.

### División: training y test

Tal como lo expresan las buenas prácticas del machine learning, vamos a separar el dataset en 2. Uno para entrenar el modelo y el otro para testearlo. La idea es que el modelo sea testeado sobre datos no vistos previamente por éste. En este caso, voy a utilizar el 80% de los registros para entrenar y el restante 20% para testear.

Para dividir el dataset, voy a crear una muestra aleatoria y replicable.

```{r}
# divido exportadores_n en expo_train y expo_test

# para poder replicar la mueastra aleatoria
set.seed(123)

# creo la muestra de un total de las filas del dataset, el 80%.
train_sample <- sample(nrow(exportadores_1), 
                       round(nrow(exportadores_1) * 0.8, 
                             digits = 0))


str(train_sample)

# creo el set de entrenamiento en base a exportadores_n, con las filas de la muestra y el set de test sin las filas de la muestra.
expo_train <- exportadores_n[train_sample,]
expo_test <- exportadores_n[-train_sample,]


```

Cuando se normalizó el dataset, se excluyó la variable target. Para entrenar el algoritmo, se necesita de esta variable, por lo que aquí la guardo en un vector para train y otro para test. Además chequeo que la proporción de "export" - "no\_export" de cada división sea similar al del dataset completo.

```{r}
expo_train_labels <- exportadores_1[train_sample, 1]
expo_test_labels <- exportadores_1[-train_sample, 1]

# Acá hequeo que la proporción de "export" o "no_export" sea similar a la del dataset completo (50%-50% aprox).

prop.table(table(expo_train_labels))

prop.table(table(expo_test_labels))

```

Bien, ahora está todo listo para entrenar el modelo.

## 3.4. Entrenando el modelo

Para un lazy learner como k-NN no es necesario ninguna contruccion de modelo. Solo hay que guardar la data de input en un formato estructurado, para usarlo con la nueva data.

La función knn() del paquete class identifica el k nearest neghbors, usando distancia Euclideana, en donde k es un numero especificado por el usuario y representa el número de vecinos a usar para decidir la clase de cada nuevo registro. El test clasifica mediante un voto entre los k nearest neighbors: se asigna la class de la mayoria de los vecinos y un empate se define aleatoreamente.

Algunos autores establecen que el número de k a utilizar debe ser igual a la raíz cuadrada del total de observaciones entrenadas.

```{r}

sqrt(nrow(expo_train)) %>% round(digits = 0)
```

En este caso, da 114. De todas maneras, este numero puede ajustarse en futuras iteraciones.

Ahora el algoritmo:

```{r}
# IMPORTANTE: cl DEBE ser vector, no table. Por lo tanto DEBE seleccionarse la columna!!!

expo_test_pred <-
        knn(
                train = expo_train,
                test = expo_test,
                cl = expo_train_labels$target,
                k = 114,
                prob = TRUE
        )

```

Esto devuelve un factor vector con los predicted labeles para el test

## 3.4. Evaluando la performance del modelo

Hacemos un cross table especificando prop.chisq = FALSE para quitar los valores chi-square del output.

```{r}
CrossTable(# nuevamente deben ser vectores
        x = expo_test_labels$target,
        y = expo_test_pred,
        prop.chisq = FALSE)

```

En esta tabla de arriba tenemos muchos valores. Las columnas representan las predicciones de clase y las filas las clases reales.

A continuación construyo una matriz de confusión un poco más casera para que me sirva para calcular el accuracy.

```{r}
# matriz de confusión casera para calcular el accuracy

confus_matrix <- table(expo_test_pred,expo_test_labels$target)

confus_matrix
```

Aquí las filas son las predicciones y las columnas representan la clasificación real.

Ahora creo una función para calcular el accuracy del modelo.

```{r}
# Esta función divide las predicciones correctas por el total de predicciones. Nos da el accuracy del modelo.

accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}

accuracy(confus_matrix)
```

Casi 76% de accuracy! Nada mal para ser el primer intento.

## 3.5. Mejorando la performance del modelo

Vamos a intentar 2 variaciones al modelo: primero, cambiando el método de normalización utilizado; segundo, probando diferentes valores de *k.*

### Normalización con z-score

Recordemos que la normalización de z-score, a diferencia de la min-max, no comprime los valores extremos hacia el centro y el rango va de valores negativos a positivos, convirtiendo la media en 0. En esta normalización, los outliers ponderan más en el cálculo de la distancia aplicado por el algoritmo que en la normalización anterior.

```{r}
# la función scale() se aplica directamente sobre un dataset

exportadores_z <- exportadores_1 %>% 
        select(-target) %>% 
        scale() %>% 
        as_tibble()
```

Para chequear que se normalizó correctamente, le doy una mirada al resumen estadístico.

```{r}
summary(exportadores_z$fob_5)
```

La media es 0 y vemos que el rango va de -0.059 a 45.603 (outlier). Un z-score menor a -3 o mayor a 3 indica un valor muy poco común. Habiendo examinado esto, aparentemente la normalización funcionó bien.

Como hicimos anteriormente, ahora debemos dividir la data normalizada en train y test sets y clasificar el test utilizando el algoritmo knn(). Finalizaremos esta iteración, comparando las predicciónes con la clase real en una matriz de confusión y aplicaremos la fórmula para conocer el accuracy del modelo.

```{r}
# ya tengo creado un registro de filas de muestra, por lo que no necesito volver a hacer realizar el set.seed() y sample()

# creo el set de entrenamiento en base a exportadores_z, con las filas de la muestra y el set de test sin las filas de la muestra.
expo_train <- exportadores_z[train_sample,]
expo_test <- exportadores_z[-train_sample,]
expo_train_labels <- exportadores_1[train_sample, 1]
expo_test_labels <- exportadores_1[-train_sample, 1]

expo_test_pred <-
        knn(
                train = expo_train,
                test = expo_test,
                cl = expo_train_labels$target,
                k = 114,
                prob = TRUE
        )

confus_matrix <- table(expo_test_pred,expo_test_labels$target)

confus_matrix

# Esta función divide las predicciones correctas por el total de predicciones. Nos da el accuracy del modelo.

accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}

accuracy(confus_matrix)

```

Aparentemente, favoreciendo el peso de los outliers (manteniendo todo lo demás igual), mejora un poco el modelo: 76,52% de accuracy.

### Valores alternativos de *k*

```{r echo = FALSE, cache = TRUE}

# función que extrae el verdader positivo de la matriz de confusión, hecha a partir del knn con un vector de valores de k
verdad_pos <- function(k) {
        sapply(k,
               function(x) {
                       expo_test_pred <- knn(
                               train = expo_train,
                               test = expo_test,
                               cl = expo_train_labels$target,
                               k = x,
                               prob = TRUE
                       )
                       confus_matrix <-
                               table(expo_test_pred, expo_test_labels$target)
                       
                       # verdadero positivo
                       confus_matrix[1, 1]
               })
        
}

# Verdadero Negativo
verdad_neg <- function(k) {
        sapply(k,
               function(x) {
                       expo_test_pred <- knn(
                               train = expo_train,
                               test = expo_test,
                               cl = expo_train_labels$target,
                               k = x,
                               prob = TRUE
                       )
                       confus_matrix <-
                               table(expo_test_pred, expo_test_labels$target)
                       
                       # Verdadero Negativo
                       confus_matrix[2, 2]
               })
        
}

# Falso negativo
falso_neg <- function(k) {
        sapply(k,
               function(x) {
                       expo_test_pred <- knn(
                               train = expo_train,
                               test = expo_test,
                               cl = expo_train_labels$target,
                               k = x,
                               prob = TRUE
                       )
                       confus_matrix <-
                               table(expo_test_pred, expo_test_labels$target)
                       
                       # Falso negativo
                       confus_matrix[1, 2]
               })
        
}

# Falso negativo
falso_pos <- function(k) {
        sapply(k,
               function(x) {
                       expo_test_pred <- knn(
                               train = expo_train,
                               test = expo_test,
                               cl = expo_train_labels$target,
                               k = x,
                               prob = TRUE
                       )
                       confus_matrix <-
                               table(expo_test_pred, expo_test_labels$target)
                       
                       # Falso negativo
                       confus_matrix[2, 1]
               })
        
}


# Accuracy con funcion
accuracy_v <- function(k) {
        sapply(k,
               function(x) {
                       expo_test_pred <- knn(
                               train = expo_train,
                               test = expo_test,
                               cl = expo_train_labels$target,
                               k = x,
                               prob = TRUE
                       )
                       confus_matrix <-
                               table(expo_test_pred, expo_test_labels$target)
                       
                       # accuracy
                       accuracy(confus_matrix) 
               })
        
}




k_alternativos <- tibble(
        k_value = c(1:140),
        verdad_pos = verdad_pos(k_value),
        verdad_neg = verdad_neg(k_value),
        falso_neg = falso_neg(k_value),
        falso_pos = falso_pos(k_value),
        accuracy_fc = accuracy_v(k_value)
) %>%
        mutate(
                tot = verdad_pos + verdad_neg + falso_neg + falso_pos,
                accuracy_m = (verdad_pos + verdad_neg) / tot
        )


k_alternativos %>% 
        ggplot(aes(k_value, accuracy_fc)) +
        geom_line(group = 1) +
        theme_minimal()
```

Iterando con diferentes valores de k, obtengo el mejor accuracy con un k igual a 13 (extraño que recomienden 114 con un dataset de 16.000...). Quizás, suceda algo diferente con la normalización min-max.

# 4. Conclusiones y disparadores

Data beats alghorithm.

Los datos a partir de los cuales se realizó el estudio son específicamente de 2014 a 2019, en donde este último solo se lo utilizó como clase a predecir. Para que sea replicable en otros años, debería crear un dataset que no tenga únicamente este periodo de años. El lapso analizado puede esconder alguna tendencia propia del mismo que imposibilite la replicabilidad para otros (futuros) años. Cuál podría ser una solución a esto?

Dandole peso a los outliers mejoró el accuracy. Qué variables serán aquellas que aportan esos outliers útiles para la performance del modelo?

Me extraña el valor de k que arroja el mejor accuracy. Muy diferente al recomendado.

Me hubiera gustado conseguir un valor superior a 80%, pero 79% no está nada mal. Más aún cuando no hice ningún ajuste en los datos.

Debería ver cómo performa en nueva data. Podría usar alguna técnica de múltiples random samples. O bien dividir el dataset en 3: uno para entrenar, otro para testear (e iterar para mejorar el accuracy) y un tercero para evaluar (una sola vez).

Armar una shiny app para que un exportador pueda autotestearse, completando algunos campos.

Disclaimer: evalua empresas con historial exportador. Aquella empresa que nunca exportó, no puede predecirse según este modelo, por no contar con atributos predictores.

Este estudio fue realizado en base a conocimientos previos de diversas fuentes, pero utilizando como guía el capítulo 3 de Machine Learning with R, de Brett Lantz. Quizás las diferencias más importantes con éste (ademas del tema y los datos, por supuesto) es la utilización del metapaquete tidyverse para muchas de las operaciones, los cálculos de la matriz de confusión y la tabla de diferentes valores de k y sus resultados, en donde aplico una función vectorizada.

Disparadores:

Usar otro algoritmo con los mismos datos para ver qué tal performa. Quizás un decision tree.

Usar otros datos con el mismo algoritmo. Hacer otro tipo del feature engeniering para obtener otro dataset a partir de los mismos datos iniciales.

Usar otros datos y otro algoritmo. Combinación de ambas
